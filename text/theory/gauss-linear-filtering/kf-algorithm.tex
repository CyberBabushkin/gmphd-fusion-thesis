We have derived several important formulas and relations, which are essential to infer the formulas and algorithm of the Kalman filter. We should recall that the filter has Gaussian-linear motion and measurement models and is essentially a Bayesian filter. The formal description of the linear models was given in \ref{eq:motion-model} and \ref{eq:measurement-model}, the prediction step of the Bayesian filter in Theorem \ref{theorem:bayes-filter-predict}, and the update step in Theorem \ref{theorem:bayes-filter-update}. With all the necessary pieces in place, we can now derive the prediction and update formulas for the Kalman filter.

\begin{theorem}[The Kalman filter prediction]\label{theorem:kalman-predict}
    Assume that the motion and measurement models are given by \ref{eq:motion-model} and \ref{eq:measurement-model}, respectively, and that the Markov assumption of the Bayes filter holds, so that the Chapman-Kolmogorov equation is applicable. Let the posterior density at time step $k-1$ be denoted as $\mathscr{N}(\vecat{x}{k-1}; \vecat{\hat{x}}{k-1|k-1}, \vecat{P}{k-1|k-1})$. Then, the predicted density is given by:

    \begin{equation}
        p\left(\vecat{x}{k} | \vecat{z}{1:k-1}\right)
        = \mathscr{N}\left(
                \vecat{x}{k};
                \mathbf{F} \vecat{\hat{x}}{k-1|k-1},
                \mathbf{F} \vecat{P}{k-1|k-1} \mathbf{F}^\intercal + \mathbf{Q}
            \right).
    \end{equation}
\end{theorem}

\begin{proof}
    \begin{align}
        p\left(\vecat{x}{k} | \vecat{z}{1:k-1}\right)
        &= \int
            p\left(
                \vecat{x}{k} | \vecat{x}{k-1}\right)
            p\left(
                \vecat{x}{k-1} | \vecat{z}{1:k-1}
            \right)
            \mathrm{d} \vecat{x}{k-1} \label{eq:kf-pred-s1} \\
        &= \int
            \mathscr{N}\left(
                \vecat{x}{k}; \mathbf{F}\vecat{x}{k-1}, \mathbf{Q}
            \right)
            \mathscr{N}\left(
                \vecat{x}{k-1}; \vecat{\hat{x}}{k-1|k-1}, \vecat{P}{k-1|k-1}
            \right)
            \mathrm{d} \vecat{x}{k-1} \label{eq:kf-pred-s2} \\
        &= \int
            \mathscr{N}\left(
                \vecat{x}{k};
                \mathbf{F} \vecat{\hat{x}}{k-1|k-1},
                \mathbf{Q} + \mathbf{F} \vecat{P}{k-1|k-1} \mathbf{F}^\intercal
            \right)
            \mathscr{N}\left(
                \vecat{x}{k-1}; \bullet, \bullet
            \right)
            \mathrm{d} \vecat{x}{k-1} \label{eq:kf-predict-gauss-id}\\
        &= \mathscr{N}\left(
                \vecat{x}{k};
                \mathbf{F} \vecat{\hat{x}}{k-1},
                \mathbf{F} \vecat{P}{k-1} \mathbf{F}^\intercal + \mathbf{Q}
            \right). \label{eq:kf-predict-result}
    \end{align}

    We started with the application of the Chapman-Kolmogorov equation from \ref{theorem:chapman-kolmogorov} and the substitution of its terms with the suitable distributions. Next, in the equation \ref{eq:kf-predict-result}, we used the result of Theorem \ref{theorem:gaussian-identity}, and finally we applied Corollary \ref{theorem:gid-integral} to get \ref{eq:kf-predict-result}.
\end{proof}

\begin{theorem}[The Kalman filter update]
    Assume that the predicted density from \ref{theorem:kalman-predict} is $p\left(\vecat{x}{k} | \vecat{z}{1:k-1}\right)=\allowbreak \mathscr{N}\left(\vecat{x}{k}; \mathbf{F} \vecat{\hat{x}}{k|k-1}, \mathbf{F} \vecat{P}{k|k-1} \mathbf{F}^\intercal + \mathbf{Q}\right)$, $p(\vecat{z}{k} | \vecat{x}{k})$ is the measurement model given in \ref{eq:measurement-model},
    and $\vecat{z}{k}$ is the measurement at time $k$. Then the posterior density after the update step is given by the following relation:

    \begin{equation}
        p(\vecat{x}{k} | \vecat{z}{k}) = 
        \mathscr{N}\left(
            \vecat{x}{k}; \vecat{\hat{x}}{k|k}, \vecat{P}{k|k}
        \right),
    \end{equation}

    where the mean and covariance are given by:

    \begin{align}
        \vecat{\hat{x}}{k|k}
        &= \vecat{\hat{x}}{k|k-1} + \vecat{K}{k}(\vecat{z}{k} - \mathbf{H}\vecat{\hat{x}}{k|k-1}), \\
        \vecat{P}{k|k}
        &= (\mathbf{I} - \vecat{K}{k}\mathbf{H})\vecat{P}{k|k-1}, \\
        \vecat{K}{k} 
        &= \vecat{P}{k|k-1}\mathbf{H}^\intercal(\mathbf{H}\vecat{P}{k|k-1}\mathbf{H}^\intercal + \mathbf{R})^{-1}.
    \end{align}
\end{theorem}

\begin{proof}
    \begin{align}
        p(\vecat{x}{k} | \vecat{z}{k})
        &= p(\vecat{z}{k} | \vecat{x}{k}) p(\vecat{x}{k}) \\
        &= p(\vecat{z}{k} | \vecat{x}{k}) p(\vecat{x}{k} | \vecat{z}{1:k-1}) \\
        &= \frac{
            \mathscr{N}\left(\vecat{z}{k}; \mathbf{H} \vecat{x}{k}, R\right)
            \mathscr{N}\left(\vecat{x}{k}; \vecat{\hat{x}}{k|k-1}, \vecat{P}{k|k-1} \right)
        }{
            \int
            \mathscr{N}\left(\vecat{z}{k}; \mathbf{H} \vecat{x}{k}, R\right)
            \mathscr{N}\left(\vecat{x}{k}; \vecat{\hat{x}}{k|k-1}, \vecat{P}{k|k-1} \right)
            \mathrm{d} \vecat{x}{k}
        } \\
        &= \frac{
            \mathscr{N}\left(\vecat{z}{k}; \mathbf{H} \vecat{\hat{x}}{k|k-1}, \mathbf{H} \vecat{P}{k|k-1} \mathbf{H}^\intercal + \mathbf{R}\right)
            \mathscr{N}\left(\vecat{x}{k}; \vecat{\hat{x}}{k|k}, \vecat{P}{k|k} \right)
        }{
            \mathscr{N}\left(\vecat{z}{k}; \mathbf{H} \vecat{\hat{x}}{k|k-1}, \mathbf{H} \vecat{P}{k|k-1} \mathbf{H}^\intercal + \mathbf{R}\right)
        } \\
        &= \mathscr{N}\left(\vecat{x}{k}; \vecat{\hat{x}}{k|k}, \vecat{P}{k|k} \right),
    \end{align}

    where, according to Theorem \ref{theorem:gaussian-identity}, the values of $\vecat{\hat{x}}{k|k}$ and $\vecat{P}{k|k}$ are:

    \begin{align*}
        \vecat{\hat{x}}{k|k}
        &= \vecat{\hat{x}}{k|k-1} + \vecat{K}{k}(\vecat{z}{k} - \mathbf{H}\vecat{\hat{x}}{k|k-1}), \\
        \vecat{P}{k|k}
        &= (\mathbf{I} - \vecat{K}{k}\mathbf{H})\vecat{P}{k|k-1}, \\
        \vecat{K}{k} 
        &= \vecat{P}{k|k-1}\mathbf{H}^\intercal(\mathbf{H}\vecat{P}{k|k-1}\mathbf{H}^\intercal + \mathbf{R})^{-1}.
    \end{align*}

    This proof utilizes the same Gaussian product identity theorem with its corollary utilizing in addition the application of the Bayes' rule.
\end{proof}

Before we conclude this section with the full algorithm of the Kalman filter, it should be noted that straightforward computation of the covariance matrix after the update step is sensitive to round-off numerical errors, and this can be avoided using a different form of the same expression for $\vecat{P}{k|k}$ \cite{bar-shalomEstimationApplicationsTracking2001}:

\begin{equation}
    \vecat{P}{k|k} =
    (\mathbf{I} - \vecat{K}{k}\mathbf{H})\vecat{P}{k|k-1}
    (\mathbf{I} - \vecat{K}{k}\mathbf{H})^\intercal
    + \vecat{K}{k} \mathbf{R} \vecat{K}{k}^\intercal.
\end{equation}

This form is called the \textit{Joseph form} and is less sensitive to numerical errors. In the algorithm, we will use this form to compute the covariance matrix after the update step. The whole algorithm of the Kalman filter is defined as follows:

\begin{algorithm}
\caption{Kalman filter algorithm}\label{alg:kf}
\begin{algorithmic}[1]
    \Procedure{KF}{$\vecat{\hat{x}}{k-1|k-1}$, $\vecat{P}{k-1|k-1}$, $\vecat{z}{k}$}
        \State $\vecat{\hat{x}}{k|k-1}, \vecat{P}{k|k-1} 
            \gets \Call{KFPredict}{\vecat{\hat{x}}{k-1|k-1}, \vecat{P}{k-1|k-1}}$
        \State $\vecat{\hat{x}}{k|k}, \vecat{P}{k|k} 
            \gets \Call{KFUpdate}{\vecat{\hat{x}}{k|k-1}, \vecat{P}{k|k-1}}$
        \State \Return $\vecat{\hat{x}}{k|k-1}, \vecat{P}{k|k-1}, \vecat{\hat{x}}{k|k}, \vecat{P}{k|k}$
    \EndProcedure
    
    \item[]
    \Procedure{KFPredict}{$\vecat{\hat{x}}{k-1|k-1}$, $\vecat{P}{k-1|k-1}$}
        \State 
            $\vecat{\hat{x}}{k|k-1} \gets \mathbf{F} \vecat{\hat{x}}{k-1|k-1}$
            \Comment{Predicted state estimate}
        \State
            $\vecat{P}{k|k-1} \gets \mathbf{F} \vecat{P}{k-1|k-1} \mathbf{F}^\intercal + \mathbf{Q}$
            \Comment{Predicted covariance}
        \State \Return $\vecat{\hat{x}}{k|k-1}, \vecat{P}{k|k-1}$
    \EndProcedure

    \item[]
    \Procedure{KFUpdate}{$\vecat{\hat{x}}{k|k-1}$, $\vecat{P}{k|k-1}$}
        \State
            $\vecat{K}{k} \gets \vecat{P}{k|k-1}\mathbf{H}^\intercal(\mathbf{H}\vecat{P}{k|k-1}\mathbf{H}^\intercal + \mathbf{R})^{-1}$
            \Comment{The Kalman gain}
        \State
            $\vecat{\hat{x}}{k|k} \gets \vecat{\hat{x}}{k|k-1} + \vecat{K}{k}(\vecat{z}{k} - \mathbf{H}\vecat{\hat{x}}{k|k-1})$
            \Comment{Posterior state estimate}
        \State
            $\vecat{P}{k|k} \gets (\mathbf{I} - \vecat{K}{k}\mathbf{H})\vecat{P}{k|k-1} (\mathbf{I} - \vecat{K}{k}\mathbf{H})^\intercal + \vecat{K}{k} \mathbf{R} \vecat{K}{k}^\intercal$
            \Comment{Posterior covariance in Joseph form}
        \State \Return $\vecat{\hat{x}}{k|k}, \vecat{P}{k|k}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

The Kalman filter is the best possible linear estimator in the MMSE (minimum mean square error) sense \cite{humpherysFreshLookKalman2012}. That means that the Kalman filter achieves the minimum expected squared error between the true and estimated values, among all possible linear estimation methods. In other words, assuming the motion model and the measurement model are linear, and the noise is Gaussian and uncorrelated, the filter provides the optimal estimate. However, as it was mentioned before, there are plenty of modifications that allow to use non-linear models or non-Gaussian correlated noise. In this work, we will use the vanilla Kalman filter.
